{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "loading_script = \"loading_script.py\"\n",
    "train_data = \"../../data/retrieve/squad-sr.json\"\n",
    "dev_data = \"../../data/squad-sr/squad-sr-dev-latin.json\"\n",
    "\n",
    "# Define model\n",
    "#model_ckpt = \"bert-base-multilingual-cased\"\n",
    "model_ckpt = \"xlm-roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional params\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = {\n",
    "    \"output_dir\": \"out\",\n",
    "    \"overwrite_output_dir\": False,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 2000,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"logging_steps\": 2000,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 1,\n",
    "    \"no_cuda\": False,\n",
    "    \"seed\": 42,\n",
    "    \"optim\": \"adamw_hf\",\n",
    "    \"optim_args\": None,\n",
    "    \"push_to_hub\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DefaultDataCollator, \\\n",
    "    AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_files = {}\n",
    "if train_data is not None:\n",
    "    data_files[\"train\"] = train_data\n",
    "if dev_data is not None:\n",
    "    data_files[\"dev\"] = dev_data\n",
    "\n",
    "# Load Dataset using loading script\n",
    "dataset = load_dataset(loading_script, data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define filter function\n",
    "def filter_dataset(example, max_length):\n",
    "    \"\"\"Remove samples with more than max_length tokens\n",
    "\n",
    "    Args:\n",
    "        example (dict): Training sample\n",
    "        max_length (int): Number of tokens\n",
    "\n",
    "    Returns:\n",
    "        list[bool]: List containing info about which samples to exclude from the dataset\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"do_not_truncate\",\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    return len(inputs[\"input_ids\"])==tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter training dataset\n",
    "filtered_train = dataset[\"train\"].filter(filter_dataset, fn_kwargs={\"max_length\": max_length})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define map function\n",
    "def preprocess_train(examples, tokenizer, max_length):\n",
    "    \"\"\"Preprocessing function for train split.\n",
    "    Convert loaded SQuAD dataset samples to representation suitable for model finetuning.\n",
    "\n",
    "    Args:\n",
    "        examples (list[dict]): Dataset samples\n",
    "        tokenizer (transformers.Tokenizer): Tokenizer\n",
    "        max_length (int): Maximum number of tokens\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Preprocessed samples\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - sequence_ids[::-1].index(1) -1\n",
    "\n",
    "        my_start_position = next((idx for idx in range(context_start, context_end+1) if offset[idx][0] > start_char), context_end)\n",
    "        start_positions.append(my_start_position - 1)\n",
    "        my_end_position = next((idx for idx in range(context_end, context_start-1, -1) if offset[idx][1] < end_char), context_start)\n",
    "        end_positions.append(my_end_position + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply map function to train dataset\n",
    "train_dataset = filtered_train.map(\n",
    "    preprocess_train,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter validation dataset\n",
    "filtered_validation = dataset[\"validation\"].filter(filter_dataset, fn_kwargs={\"max_length\": max_length})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation(examples, tokenizer, max_length):\n",
    "    \"\"\"Preprocess samples in validation dataset.\n",
    "    Differs from `preprocess_train` because processed samples contain `offset_mapping` and `example_id` values\n",
    "\n",
    "    Args:\n",
    "        examples (list[dict]): Samples to process\n",
    "        tokenizer (transformers.Tokenizer): Tokenizer\n",
    "        max_length (int): Maximum tokenized sequence length\n",
    "\n",
    "    Returns:\n",
    "        Dataset: Processed dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs[\"offset_mapping\"]\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        example_ids.append(examples[\"id\"][i])\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        context_start = sequence_ids.index(1)\n",
    "        context_end = len(sequence_ids) - sequence_ids[::-1].index(1) -1\n",
    "\n",
    "        my_start_position = next((idx for idx in range(context_start, context_end+1) if offset[idx][0] > start_char), context_end)\n",
    "        start_positions.append(my_start_position - 1)\n",
    "        my_end_position = next((idx for idx in range(context_end, context_start-1, -1) if offset[idx][1] < end_char), context_start)\n",
    "        end_positions.append(my_end_position + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to validation dataset\n",
    "validation_dataset = filtered_validation.map(\n",
    "    preprocess_validation,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": max_length}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "collator = DefaultDataCollator()\n",
    "\n",
    "training_args = TrainingArguments(**training_args)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset= validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9228a2d60ec10907bce5bff9bcb9ef094edbbac38ba30f1458adf98cbaefdbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
